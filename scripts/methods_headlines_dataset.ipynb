{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Million News Headline dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we run NMF, NCPD, LDA, and online NCPD as dynamic topic modeling techniques for the Australian Broadcasting Corporation (ABC) headlines dataset ('A Million News Headline' dataset downloaded from [Kaggle](https://www.kaggle.com/therohk/million-headlines)).\n",
    "\n",
    "The dataset consists of a million news headlines published over a period of seventeen years in ABC and covers international news along with Australian news from early-2003 to end-2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "4erDBMWTyQ_D",
    "outputId": "7423bfa8-d24d-43a7-d03d-dbf563a85e9a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import tensorly as tl\n",
    "import nltk\n",
    "import collections\n",
    "import gensim\n",
    "from gensim import corpora,models\n",
    "from nltk.corpus import stopwords\n",
    "from tensorly import fold\n",
    "from tensorly.decomposition import non_negative_parafac\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from config import data_dir, results_dir\n",
    "from covid19 import plotting, utils\n",
    "from covid19.online_CPDL.ocpdl import Online_CPDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 25\n",
    "n_top_words = 5 # number of keywords to display for each topic\n",
    "save_figures = True # True to save results; False otherwise\n",
    "load_NMF_factors = False # True for loading pre-saved NMF factors; False for computing the factors\n",
    "load_NCPD_factors = False # True for loading pre-saved NCPD factors; False for computing the factors\n",
    "load_lda_model = False # True for loading trained LDA model; False for training a model\n",
    "load_ONCPD_factors = False # True for loading pre-saved ONCPD factors; False for computing the factors\n",
    "local_path = results_dir\n",
    "directory = os.path.join(\n",
    "            data_dir,\n",
    "            \"abcnews-date-text.csv\",\n",
    "        ) # Fill\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load ABC news dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(directory, error_bad_lines=False, warn_bad_lines = True);\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the date of publishing\n",
    "data['publish_date'] = pd.to_datetime(data['publish_date'],format='%Y%m%d')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"Number of headlines {len(data)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomly subsample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsample dataset per month\n",
    "data_subsample = data.copy()\n",
    "sample_val = 700 # number of headlines per month\n",
    "\n",
    "sub_indices = data['publish_date'].groupby([data.publish_date.dt.year, data.publish_date.dt.month]).sample(n=sample_val, replace = False).index\n",
    "\n",
    "data_subsample = data_subsample.iloc[sub_indices]\n",
    "print(f\"Number of headlines in the downsampled dataset {len(data_subsample)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort data by chronological order \n",
    "data_subsample.sort_values(by=['publish_date'], inplace=True)\n",
    "data_subsample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check min/max number of headlines sampled per month\n",
    "dfg = data_subsample.groupby([data.publish_date.dt.year, data.publish_date.dt.month])\n",
    "print(dfg.count().min())\n",
    "print(dfg.count().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a yyyy-mm column\n",
    "data_subsample['month_year'] = pd.to_datetime(data_subsample['publish_date']).dt.to_period('M')\n",
    "data_subsample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test example for monthly subsampling\n",
    "len(data_subsample[data_subsample[\"month_year\"] == pd.to_datetime('200709', format='%Y%m').to_period('M')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the headline text in the dataframe into a corpus (list)\n",
    "corpus = data_subsample['headline_text'].values.tolist()\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract TF-IDF weights and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aDm2Q36G1MKu"
   },
   "outputs": [],
   "source": [
    "# Vary the choices of the parameters of TfidfVectorizer\n",
    "n_features = 7000\n",
    "max_df=0.7\n",
    "min_df=5\n",
    "stop_words_list = nltk.corpus.stopwords.words('english')\n",
    "stop_words_list.append(\"abc\") # remove the common string \"abc\" which could stand for \"Australian Broadcasting Corporation\"\n",
    "vectorizer = TfidfVectorizer(max_df=max_df, \n",
    "                                min_df=min_df, \n",
    "                                #token_pattern = '[a-zA-Z]+',\n",
    "                                max_features=n_features,\n",
    "                                #ngram_range = (1,2),\n",
    "                                stop_words=stop_words_list)\n",
    "vectors = vectorizer.fit_transform(corpus) \n",
    "feature_names = vectorizer.get_feature_names()\n",
    "dense = vectors.todense()\n",
    "denselist = np.array(dense).transpose() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Shape of the matrix {}.\".format(denselist.shape))\n",
    "print(\"Dimensions: (vocabulary, headlines)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize data into a third-order tensor\n",
    "num_time_slices = dfg.ngroups # number of time slices\n",
    "n_div = 1 # number of months per time slice\n",
    "n_headlines = sample_val*n_div # number of headlines per time slice\n",
    "data_tens = fold(denselist, 1, (denselist.shape[1] // n_headlines, denselist.shape[0], n_headlines))\n",
    "\n",
    "print(\"Shape of the tensor {}.\".format(data_tens.shape))\n",
    "print(\"Dimensions: (time, vocabulary, headlines)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all features/words extracted\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\", font_scale=1.001, context=\"talk\")\n",
    "\n",
    "def heatmap(\n",
    "    data,\n",
    "    x_tick_labels=None,\n",
    "    x_label=\"\",\n",
    "    y_tick_labels=None,\n",
    "    y_label=\"\",\n",
    "    figsize=(7, 9),\n",
    "    max_data=None,\n",
    "):\n",
    "    \"\"\"Plot heatmap.\n",
    "    Args:\n",
    "        data: (2d array) data to be plotted (topics x date)\n",
    "        x_tick_labels (list of str)\n",
    "    Returns:\n",
    "        fig\n",
    "        ax\n",
    "    \"\"\"\n",
    "\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    ax = sns.heatmap(\n",
    "        data,\n",
    "        rasterized=True,\n",
    "        vmax=max_data,\n",
    "        cbar_kws=dict(use_gridspec=False, location=\"top\", aspect =80),\n",
    "        )\n",
    "\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(np.arange(0, data.shape[0], 1.0) + 0.5, rotation=0)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    \n",
    "    ax.set_xticks(np.arange(0, data.shape[1], 12))\n",
    "    ax.set_xticklabels([str(x) for x in range(2003, 2020)])\n",
    "\n",
    "    if y_tick_labels is None:\n",
    "        y_tick_labels = [topic_num + 1 for topic_num in range(data.shape[0])]\n",
    "    ax.set_yticklabels(y_tick_labels)\n",
    "\n",
    "    #if x_tick_labels is not None:\n",
    "     #   labels = [x_tick_labels[int(item.get_text())] for item in ax.get_xticklabels()]\n",
    "     #   ax.set_xticklabels(labels)\n",
    "\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run NMF\n",
    "if load_NMF_factors:\n",
    "    W, H = pickle.load(open(os.path.join(local_path,\"NMF_factors_headlines.pickle\"), \"rb\"))\n",
    "else:\n",
    "    nmf = NMF(n_components=rank, init='nndsvd')\n",
    "    W = nmf.fit_transform(denselist) # Dictionary\n",
    "    H = nmf.components_ # Topic representations\n",
    "    NMF_factors = W,H\n",
    "    with open(os.path.join(local_path,\"NMF_factors_headlines.pickle\"), \"wb\") as f:\n",
    "        pickle.dump(NMF_factors, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display topics\n",
    "for topic_idx, topic in enumerate(W.T):\n",
    "    message = \"Topic %d: \" % (topic_idx+1)\n",
    "    message += \", \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic Keywords representations\n",
    "num_keywords = 3\n",
    "topics_freqs = []\n",
    "for i, topic in enumerate(W.T):\n",
    "    topics_freqs.append(\n",
    "        {feature_names[i]: topic[i] for i in reversed(topic.argsort()[-20:])}\n",
    "    )\n",
    "\n",
    "# Make word and frequency lists for each topic.\n",
    "sorted_topics = [\n",
    "    sorted(topic.items(), key=lambda item: item[1], reverse=True)\n",
    "    for topic in topics_freqs\n",
    "]\n",
    "word_lists = [[item[0] for item in topic[:num_keywords]] for topic in sorted_topics]\n",
    "freq_lists = [[item[1] for item in topic[:num_keywords]] for topic in sorted_topics]\n",
    "\n",
    "y_tick_labels = [\n",
    "    \"{}: {}\".format(\", \".join(word_lists[i][0:3]), i + 1) for i in range(rank)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get topic distributions for each time slice\n",
    "topics_over_time = np.split(H, num_time_slices, axis=1)\n",
    "\n",
    "# Average topic distributions over time.\n",
    "avg_topics_over_time = [np.mean(topics, axis=1) for topics in topics_over_time]\n",
    "\n",
    "# Normalize to get the distribution over topics for each day.\n",
    "avg_topics_over_time = np.array(\n",
    "    [topics / np.sum(topics) for topics in avg_topics_over_time]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize topic distributions\n",
    "date_strs = [str(x+1) for x in range(data_tens.shape[0])] \n",
    "#y_tick_labels = [\"{}\".format(i + 1) for i in range(rank)]\n",
    "\n",
    "fig, ax = heatmap(\n",
    "    avg_topics_over_time.T,\n",
    "    x_tick_labels=date_strs,\n",
    "    x_label=\"Year\",\n",
    "    y_tick_labels=y_tick_labels,\n",
    "    y_label=\"Topic\",\n",
    "    figsize= (10, 8)\n",
    ")\n",
    "\n",
    "if save_figures:\n",
    "    plt.savefig(os.path.join(local_path,\"NMF_Normalized_Topic_Time_Headlines.png\"), bbox_inches = 'tight', pad_inches = 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run NCPD on Tensor data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_NCPD_factors:\n",
    "    factors = pickle.load(open(os.path.join(local_path,\"NCPD_factors_headlines.pickle\"), \"rb\"))\n",
    "    weights = pickle.load(open(os.path.join(local_path,\"NCPD_weights_headlines.pickle\"), \"rb\"))\n",
    "else:\n",
    "    factors = non_negative_parafac(data_tens, rank=rank, init = 'svd') \n",
    "    if len(factors) == 2:\n",
    "        weights, factors = factors\n",
    "        print(weights)\n",
    "        with open(os.path.join(local_path,\"NCPD_weights_headlines.pickle\"), \"wb\") as f:\n",
    "            pickle.dump(weights, f)\n",
    "    with open(os.path.join(local_path,\"NCPD_factors_headlines.pickle\"), \"wb\") as f:\n",
    "        pickle.dump(factors, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensions of the factor matrices\n",
    "for i in range(len(factors)):\n",
    "    print(factors[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display Topics\n",
    "for topic_idx, topic in enumerate(factors[1].T):\n",
    "    message = \"Topic %d: \" % (topic_idx+1)\n",
    "    message += \", \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic Keywords representations\n",
    "num_keywords = 3\n",
    "topics_freqs = []\n",
    "for i, topic in enumerate(factors[1].T):\n",
    "    topics_freqs.append(\n",
    "        {feature_names[i]: topic[i] for i in reversed(topic.argsort()[-20:])}\n",
    "    )\n",
    "\n",
    "# Make word and frequency lists for each topic.\n",
    "sorted_topics = [\n",
    "    sorted(topic.items(), key=lambda item: item[1], reverse=True)\n",
    "    for topic in topics_freqs\n",
    "]\n",
    "word_lists = [[item[0] for item in topic[:num_keywords]] for topic in sorted_topics]\n",
    "freq_lists = [[item[1] for item in topic[:num_keywords]] for topic in sorted_topics]\n",
    "\n",
    "y_tick_labels = [\n",
    "    \"{}: {}\".format(\", \".join(word_lists[i][0:3]), i + 1) for i in range(rank)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plotting normalized temporal factor\n",
    "time_norm = factors[0]/factors[0].sum(axis=1)[:,None] # row sum equal to 1\n",
    "\n",
    "fig, ax = heatmap(\n",
    "    time_norm.T, #factors[0].T for un-normalized factor\n",
    "    x_tick_labels=date_strs,\n",
    "    x_label=\"Year\",\n",
    "    y_tick_labels=y_tick_labels,\n",
    "    y_label=\"Topic\",\n",
    "    figsize=(10, 8)\n",
    ")\n",
    "\n",
    "if save_figures:\n",
    "    plt.savefig(os.path.join(local_path,\"NCPD_Normalized_Topic_Time_Headlines.png\"), bbox_inches = 'tight', pad_inches = 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA + Temporal evolution extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "tokenized_docs = list(map(vectorizer.build_analyzer(),corpus)) # pre-process and tokenize documents using tfidfvectorizer\n",
    "# Filter processed_docs based on features list\n",
    "processed_docs = []\n",
    "for doc in tokenized_docs:\n",
    "    processed_docs.append([wd_str for wd_str in doc if(wd_str in feature_names)])\n",
    "                              \n",
    "dictionary = gensim.corpora.Dictionary(processed_docs) # define a (gensim) dictionary\n",
    "print(len(dictionary))\n",
    "\n",
    "# Create bag-of-words corpus (gensim format)\n",
    "corpus=[dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative Preprocessing (to create a separate dictionary from NMF/NCPD)\n",
    "\"\"\"\n",
    "# Data Preprocessing\n",
    "processed_docs = list(map(vectorizer.build_analyzer(),corpus)) # pre-process and tokenize documents using tfidfvectorizer\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs) # define a (gensim) dictionary\n",
    "dictionary.filter_extremes(no_below=min_df,no_above=max_df,keep_n=n_features) # use the same parameters as tfidfvectorizer\n",
    "print(len(dictionary))\n",
    "\n",
    "# Create bag-of-words corpus (gensim format)\n",
    "corpus=[dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if dictionaries are the same\n",
    "collections.Counter(list(dictionary.values())) == collections.Counter(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run LDA\n",
    "run_LDA_tfidf = False\n",
    "\n",
    "if run_LDA_tfidf:\n",
    "    # Convert BOW Corpus to TFIDF Corpus\n",
    "    tfidf=models.TfidfModel(corpus)\n",
    "    corpus=tfidf[corpus]   \n",
    "\n",
    "if load_lda_model:\n",
    "    lda_model = pickle.load(open(os.path.join(local_path,\"lda_model_headlines.pickle\"), \"rb\"))\n",
    "else:\n",
    "    lda_model = gensim.models.LdaMulticore(corpus,\n",
    "                                         num_topics=rank, \n",
    "                                         id2word = dictionary, \n",
    "                                         passes = 20,\n",
    "                                         workers=4,\n",
    "                                         minimum_probability = 0,\n",
    "                                         random_state = 1\n",
    "                                          )\n",
    "    \n",
    "    with open(os.path.join(local_path,\"lda_model_headlines.pickle\"), \"wb\") as f:\n",
    "        pickle.dump(lda_model, f)                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display topics\n",
    "for topic_idx, topic in enumerate(lda_model.get_topics()):\n",
    "    message = \"Topic %d: \" % (topic_idx+1)\n",
    "    message += \", \".join([dictionary[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic Keywords representations\n",
    "num_keywords = 3\n",
    "topics_freqs = []\n",
    "for i, topic in enumerate(lda_model.get_topics()):\n",
    "    topics_freqs.append(\n",
    "        {dictionary[i]: topic[i] for i in reversed(topic.argsort()[-20:])}\n",
    "    )\n",
    "\n",
    "# Make word and frequency lists for each topic.\n",
    "sorted_topics = [\n",
    "    sorted(topic.items(), key=lambda item: item[1], reverse=True)\n",
    "    for topic in topics_freqs\n",
    "]\n",
    "word_lists = [[item[0] for item in topic[:num_keywords]] for topic in sorted_topics]\n",
    "freq_lists = [[item[1] for item in topic[:num_keywords]] for topic in sorted_topics]\n",
    "\n",
    "y_tick_labels = [\n",
    "    \"{}: {}\".format(\", \".join(word_lists[i][0:3]), i + 1) for i in range(rank)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA representation matrix (topics by documents)\n",
    "lda_rep_mat = np.empty((rank, len(corpus)))\n",
    "for numb in range(len(corpus)):\n",
    "    for index, score in lda_model.get_document_topics(corpus[numb], minimum_probability = 0):\n",
    "        lda_rep_mat[index, numb] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get topic distributions for each time slice\n",
    "lda_topics_over_time = np.split(lda_rep_mat, num_time_slices, axis=1)\n",
    "\n",
    "# Average topic distributions over time.\n",
    "lda_avg_topics_over_time = [np.mean(topics, axis=1) for topics in lda_topics_over_time]\n",
    "\n",
    "# Normalize to get the distribution over topics for each day.\n",
    "lda_avg_topics_over_time = np.array(\n",
    "    [topics / np.sum(topics) for topics in lda_avg_topics_over_time]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Visualize topic distributions\n",
    "fig, ax = heatmap(\n",
    "    lda_avg_topics_over_time.T,\n",
    "    x_tick_labels=date_strs,\n",
    "    x_label=\"Year\",\n",
    "    y_tick_labels=y_tick_labels,\n",
    "    y_label=\"Topic\",\n",
    "    figsize=(10, 8)\n",
    ")\n",
    "\n",
    "if save_figures:\n",
    "    plt.savefig(os.path.join(local_path,\"LDA_Topic_Time_Headlines.png\"), bbox_inches = 'tight', pad_inches = 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online NCPD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVD intialization (Optional)\n",
    "weights, factors = non_negative_parafac(data_tens, rank=rank, init = 'svd', n_iter_max=1) \n",
    "print(factors[0].shape)\n",
    "print(factors[1].shape)\n",
    "loading = {}\n",
    "loading.update({'U0':factors[0]})\n",
    "loading.update({'U1':factors[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Online NCPD\n",
    "if load_ONCPD_factors:\n",
    "    factors = pickle.load(open(os.path.join(local_path,\"20news_ONCPD_factors.pickle\"), \"rb\"))\n",
    "else:\n",
    "    OCPDL_model = Online_CPDL(X=data_tens,\n",
    "                        batch_size=data_tens.shape[-1]//10, # use only 1/10 documents per minibatch\n",
    "                        iterations=20,\n",
    "                        n_components=rank,\n",
    "                        ini_loading=loading,\n",
    "                        ini_A=None,\n",
    "                        ini_B=None,\n",
    "                        alpha=0,\n",
    "                        beta=1,\n",
    "                        subsample=True)\n",
    "    \n",
    "    result_dict = OCPDL_model.train_dict(if_compute_recons_error=False,\n",
    "                                   save_folder=None,\n",
    "                                   output_results=False)\n",
    "    loading = result_dict.get(\"loading\")\n",
    "    factors = []\n",
    "    for i in loading.keys():\n",
    "        factors.append(loading.get(str(i)))\n",
    "    \n",
    "    print('!!! X.shape', data_tens.shape)\n",
    "    with open(os.path.join(local_path,\"20news_ONCPD_factors.pickle\"), \"wb\") as f:\n",
    "        pickle.dump(factors, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the shape of the factor matrices\n",
    "for i in range(len(factors)):\n",
    "    print(factors[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display topics\n",
    "for topic_idx, topic in enumerate(factors[1].T):\n",
    "    message = \"Topic %d: \" % (topic_idx+1)\n",
    "    message += \", \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize topic distributions\n",
    "date_strs = [str(x+1) for x in range(data_tens.shape[0])]\n",
    "\n",
    "time_norm = factors[0]/factors[0].sum(axis=1)[:,None] # row sum equal to 1\n",
    "y_tick_labels = [\"{}\".format(i + 1) for i in range(rank)]\n",
    "\n",
    "fig, ax = heatmap(\n",
    "    time_norm.T, #factors[2].T for un-normalized factor\n",
    "    x_tick_labels=date_strs,\n",
    "    x_label=\"Time\",\n",
    "    y_tick_labels=y_tick_labels,\n",
    "    y_label=\"\",\n",
    "    figsize = (6,4),\n",
    ")\n",
    "\n",
    "\n",
    "if save_figures:\n",
    "    plt.savefig(os.path.join(local_path,\"ONCPD_Normalized_Topic_Time_Headlines.png\"), bbox_inches = 'tight', pad_inches = 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "20news_new_example.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
